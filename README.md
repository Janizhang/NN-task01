## 构建两层神经网络对数据集MINIST进行分类
### 1、训练
* 定义激活函数sigmoid、sigmoid函数的导数以及softmax函数
* 初始化模型参数，反向传播函数返回梯度
* 在损失计算函数中，加上 $L_2$ 正则化项 
* train函数中，采用学习率下降策略和优化器SGD
  * 虽然采用学习率衰减的方法能让模型收敛的更好，但是如果遇到鞍点的时候，模型就没法继续收敛，如果学习率此时很小，那将永远    无法走出鞍点；为了解决这一问题，我采用Cyclical Learning Rates(CRL)的方法，设置max_lr与base_lr这两个参数，让      学习率在这两个数之间变化，且max_lr会随时间衰减，从而达到学习率下降的目的
  * 在优化器SGD中，我采用的是mini-batch方法，每次随机获取小批量数据，大小由batch_size来指定，利用这些数据进行梯度更新

### 2、参数查找：学习率，隐藏层大小，正则化强度
